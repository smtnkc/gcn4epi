{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN data requirements:\n",
    "\n",
    "https://github.com/kimiyoung/planetoid#prepare-the-data\n",
    "\n",
    "https://github.com/tkipf/gcn#data\n",
    "\n",
    "|  |  |\n",
    "| :-- | :-- |\n",
    "| ind.GM12878.x | the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object |\n",
    "| ind.GM12878.tx | the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object |\n",
    "| ind.GM12878.allx | the feature vectors of both labeled and unlabeled training instances (a superset of ind.GM12878.x) as scipy.sparse.csr.csr_matrix object |\n",
    "| ind.GM12878.y | the one-hot labels of the labeled training instances as numpy.ndarray object |\n",
    "| ind.GM12878.ty | the one-hot labels of the test instances as numpy.ndarray object |\n",
    "| ind.GM12878.ally | the labels for instances in ind.GM12878.allx as numpy.ndarray object |\n",
    "| ind.GM12878.graph | a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict object |\n",
    "| ind.GM12878.test.index | the indices of test instances in graph, for the inductive setting as list object |\n",
    "\n",
    "**All objects above must be saved using python pickle module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "CELL_LINE = 'GM12878'\n",
    "K_MER = 5\n",
    "TEST_EID = 981 # arbitrary enhancer id to be used tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new DF where each element is a tuple of 3 elements: (id, name, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnhancerById(df_ep, eid):\n",
    "    for row in range(len(df_ep)):\n",
    "        enh = df_ep['enhancer'][row]\n",
    "        if enh[0] == eid:\n",
    "            return enh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enhancer</th>\n",
       "      <th>promoter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1265, GM12878|chr1:9685722-9686400, TGACA GAC...</td>\n",
       "      <td>(1266, GM12878|chr1:9747084-9749721, TTTTG TTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1199, GM12878|chr1:24136556-24136600, GTGGC T...</td>\n",
       "      <td>(1205, GM12878|chr1:24193468-24194871, TGAAT G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1200, GM12878|chr1:24136600-24136932, GAAAC A...</td>\n",
       "      <td>(1205, GM12878|chr1:24193468-24194871, TGAAT G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1201, GM12878|chr1:24137625-24137875, GTGCC T...</td>\n",
       "      <td>(1205, GM12878|chr1:24193468-24194871, TGAAT G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1202, GM12878|chr1:24139145-24139414, GCCCA C...</td>\n",
       "      <td>(1205, GM12878|chr1:24193468-24194871, TGAAT G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            enhancer  \\\n",
       "0  (1265, GM12878|chr1:9685722-9686400, TGACA GAC...   \n",
       "1  (1199, GM12878|chr1:24136556-24136600, GTGGC T...   \n",
       "2  (1200, GM12878|chr1:24136600-24136932, GAAAC A...   \n",
       "3  (1201, GM12878|chr1:24137625-24137875, GTGCC T...   \n",
       "4  (1202, GM12878|chr1:24139145-24139414, GCCCA C...   \n",
       "\n",
       "                                            promoter  \n",
       "0  (1266, GM12878|chr1:9747084-9749721, TTTTG TTT...  \n",
       "1  (1205, GM12878|chr1:24193468-24194871, TGAAT G...  \n",
       "2  (1205, GM12878|chr1:24193468-24194871, TGAAT G...  \n",
       "3  (1205, GM12878|chr1:24193468-24194871, TGAAT G...  \n",
       "4  (1205, GM12878|chr1:24193468-24194871, TGAAT G...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ep_sentences = pd.read_csv('data/' + CELL_LINE + '/ep_sentences_' + str(K_MER) + 'mer.csv')\n",
    "\n",
    "e_list = []\n",
    "p_list = []\n",
    "\n",
    "for i in range(len(df_ep_sentences)):\n",
    "    e_list.append( (df_ep_sentences['enhancer_name'][i], df_ep_sentences['enhancer_sentence'][i]) )\n",
    "    p_list.append( (df_ep_sentences['promoter_name'][i], df_ep_sentences['promoter_sentence'][i]) )\n",
    "\n",
    "x_list = sorted(list(set(list(df_ep_sentences['enhancer_name']) + list(df_ep_sentences['promoter_name']))))\n",
    "\n",
    "id_dict = {}\n",
    "chr_id = 0\n",
    "for x in x_list:\n",
    "    id_dict[x] = chr_id\n",
    "    chr_id += 1\n",
    "    \n",
    "#print(len(id_dict), 'keys in dictionary\\n')\n",
    "\n",
    "for i in range(len(e_list)):\n",
    "    e_list[i] = (id_dict[e_list[i][0]], ) + e_list[i]\n",
    "    \n",
    "for i in range(len(p_list)):\n",
    "    p_list[i] = (id_dict[p_list[i][0]], ) + p_list[i]\n",
    "\n",
    "df_ep = pd.DataFrame({'enhancer': e_list, 'promoter': p_list})\n",
    "df_ep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981, 'GM12878|chr19:39930827-39930919', 'ACAAA CAAAT AAATG AATGA ATGAT TGATG GATGA ATGAT TGATG GATGA ATGAT TGATG GATGA ATGAC TGACT GACTA ACTAC CTACA TACAG ACAGC CAGCT AGCTG GCTGC CTGCA TGCAT GCATG CATGT ATGTA TGTAA GTAAA TAAAT AAATA AATAG ATAGT TAGTG AGTGT GTGTT TGTTT GTTTA TTTAC TTACT TACTC ACTCT CTCTG TCTGT CTGTG TGTGC GTGCC TGCCA GCCAG CCAGG CAGGT AGGTA GGTAT GTATT TATTG ATTGG TTGGT TGGTT GGTTT GTTTA TTTAA TTAAA TAAAT AAATG AATGC ATGCT TGCTT GCTTT CTTTA TTTAA TTAAG TAAGT AAGTA AGTAT GTATG TATGT ATGTT TGTTA GTTAG TTAGC TAGCT AGCTT GCTTA CTTAT TTATT TATTT ATTTA TTTAC') \n",
      "\n",
      "Test sentence length = 89\n"
     ]
    }
   ],
   "source": [
    "test_enh = getEnhancerById(df_ep, TEST_EID)\n",
    "print(test_enh, '\\n')\n",
    "print('Test sentence length =', len(test_enh[2].split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE ADJACENCY MATRIX (NxN)\n",
    "\n",
    "N = Number of nodes (enhancers + promoters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samet/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2668x2668 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 4226 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "adj = csr_matrix((len(id_dict), len(id_dict)), dtype=np.longlong)\n",
    "\n",
    "for i in range(len(df_ep)):\n",
    "    x = df_ep['enhancer'][i][0]\n",
    "    y = df_ep['promoter'][i][0]\n",
    "    adj[x,y] = 1\n",
    "    adj[y,x] = 1\n",
    "\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancer 981 -> Promoter 983\n"
     ]
    }
   ],
   "source": [
    "print('Enhancer', TEST_EID, '-> Promoter', adj[TEST_EID].indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE FEATURE VECTORS (NxD)\n",
    "\n",
    "N = Number of nodes (enhancers + promoters)\n",
    "\n",
    "D = Number of words in vocabulary (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST FOR ENHANCER 981 \n",
      "\n",
      "Length = 89 \n",
      "\n",
      "ACAAA CAAAT AAATG AATGA ATGAT TGATG GATGA ATGAT TGATG GATGA ATGAT TGATG GATGA ATGAC TGACT GACTA ACTAC CTACA TACAG ACAGC CAGCT AGCTG GCTGC CTGCA TGCAT GCATG CATGT ATGTA TGTAA GTAAA TAAAT AAATA AATAG ATAGT TAGTG AGTGT GTGTT TGTTT GTTTA TTTAC TTACT TACTC ACTCT CTCTG TCTGT CTGTG TGTGC GTGCC TGCCA GCCAG CCAGG CAGGT AGGTA GGTAT GTATT TATTG ATTGG TTGGT TGGTT GGTTT GTTTA TTTAA TTAAA TAAAT AAATG AATGC ATGCT TGCTT GCTTT CTTTA TTTAA TTAAG TAAGT AAGTA AGTAT GTATG TATGT ATGTT TGTTA GTTAG TTAGC TAGCT AGCTT GCTTA CTTAT TTATT TATTT ATTTA TTTAC\n"
     ]
    }
   ],
   "source": [
    "merged_list = list(set(list(df_ep['enhancer']) + list(df_ep['promoter'])))\n",
    "merged_list = sorted(merged_list) # sort by first element (id)\n",
    "\n",
    "corpus = []\n",
    "for t in merged_list:\n",
    "    corpus.append(t[2])\n",
    "\n",
    "print('TEST FOR ENHANCER', TEST_EID, '\\n')\n",
    "print('Length =', len(corpus[TEST_EID].split(' ')), '\\n')\n",
    "print(corpus[TEST_EID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTS FOR ENHANCER 981 \n",
      "\n",
      "[225, 691, 231, 718, 830, 236, 73, 172, 590, 903, 315, 807, 786, 119, 779, 568, 719, 827, 967, 814, 239, 764, 927, 499, 754, 203, 50, 478, 44, 959, 187, 751, 975, 956, 1003, 250, 57, 910, 227, 56, 969, 295, 540, 916, 741, 113, 158, 299, 14, 704, 915, 484, 330, 953, 703, 943, 633, 179, 508, 639, 1009, 252, 831, 12, 259, 797, 636, 64, 452, 962, 771, 960, 1008, 944, 594, 494, 891, 159] \n",
      "\n",
      "[12, 14, 540, 44, 50, 568, 56, 57, 64, 73, 590, 594, 113, 119, 633, 636, 639, 158, 159, 172, 691, 179, 187, 703, 704, 203, 718, 719, 225, 227, 741, 231, 236, 751, 239, 754, 250, 764, 252, 259, 771, 779, 786, 797, 295, 807, 299, 814, 315, 827, 830, 831, 330, 891, 903, 910, 915, 916, 927, 943, 944, 953, 956, 959, 960, 962, 452, 967, 969, 975, 478, 484, 1003, 494, 1008, 1009, 499, 508] \n",
      "\n",
      "Number of unique words in sentence: 78 vs 78\n",
      "Comparison of sets of unique words: True \n",
      "\n",
      "  (0, 225)\t0.09889583756087965\n",
      "  (0, 691)\t0.10906175037204857\n",
      "  (0, 231)\t0.09277112814377503\n",
      "  (0, 718)\t0.11188129784919433\n",
      "  (0, 830)\t0.10418894963948112\n",
      "  (0, 236)\t0.10282575354175785\n",
      "  (0, 73)\t0.0861828934717262\n",
      "  (0, 172)\t0.09800855728077333\n",
      "  (0, 590)\t0.09263927883772234\n",
      "  (0, 903)\t0.0894098857483894\n",
      "  (0, 315)\t0.09070209452100499\n",
      "  (0, 807)\t0.09460874819399956\n",
      "  (0, 786)\t0.0881100695267997\n",
      "  (0, 119)\t0.08731448913446836\n",
      "  (0, 779)\t0.10087396666984118\n",
      "  (0, 568)\t0.28192097379150155\n",
      "  (0, 719)\t0.09670643310930542\n",
      "  (0, 827)\t0.10020587105394742\n",
      "  (0, 967)\t0.0952982597624137\n",
      "  (0, 814)\t0.10186673148660105\n",
      "  (0, 239)\t0.08859355581601921\n",
      "  (0, 764)\t0.19591944369571135\n",
      "  (0, 927)\t0.084364331217082\n",
      "  (0, 499)\t0.10342099345810811\n",
      "  (0, 754)\t0.10513758240642894\n",
      "  :\t:\n",
      "  (0, 953)\t0.08767093713379748\n",
      "  (0, 703)\t0.08622149134068006\n",
      "  (0, 943)\t0.08932765135800957\n",
      "  (0, 633)\t0.08336968301939135\n",
      "  (0, 179)\t0.10513758240642894\n",
      "  (0, 508)\t0.08916358656758067\n",
      "  (0, 639)\t0.08403058204230904\n",
      "  (0, 1009)\t0.18713919566841328\n",
      "  (0, 252)\t0.08887776016806\n",
      "  (0, 831)\t0.08015578763168828\n",
      "  (0, 12)\t0.08119269283958012\n",
      "  (0, 259)\t0.08511405678278582\n",
      "  (0, 797)\t0.1036939136566414\n",
      "  (0, 636)\t0.10325795240529898\n",
      "  (0, 64)\t0.08036145828004834\n",
      "  (0, 452)\t0.09488341469137822\n",
      "  (0, 962)\t0.09451752643382595\n",
      "  (0, 771)\t0.17914952147354837\n",
      "  (0, 960)\t0.08289773351557139\n",
      "  (0, 1008)\t0.16747154375072412\n",
      "  (0, 944)\t0.0887557607593996\n",
      "  (0, 594)\t0.07951001423386207\n",
      "  (0, 494)\t0.07781670254657287\n",
      "  (0, 891)\t0.07954379550494979\n",
      "  (0, 159)\t0.08672598961888328\n"
     ]
    }
   ],
   "source": [
    "print('TESTS FOR ENHANCER', TEST_EID, '\\n')\n",
    "\n",
    "vector = list(features[TEST_EID].indices)\n",
    "print(vector, '\\n') # vector tokens are not ordered\n",
    "\n",
    "# Get sentence with id = test_id from df\n",
    "test_enh = getEnhancerById(df_ep, TEST_EID)\n",
    "sent = test_enh[2]\n",
    "\n",
    "tokens = []\n",
    "for word in sent.split(' '):\n",
    "    tokens.append(vectorizer.vocabulary_[word.lower()])\n",
    "    # used lower() because vectorizer keys are lowercased (e.g. aactg)\n",
    "\n",
    "tokens = list(set(tokens)) # since vector does not have duplicated tokens\n",
    "print(tokens, '\\n')\n",
    "\n",
    "print('Number of unique words in sentence:', len(vector), \"vs\", len(tokens))\n",
    "print('Comparison of sets of unique words:', sorted(vector) == sorted(tokens), '\\n')\n",
    "\n",
    "print(features[TEST_EID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCER 384 number of unique words 281 =? 281\n",
      "ENHANCER 1204 number of unique words 102 =? 102\n",
      "ENHANCER 1145 number of unique words 19 =? 19\n",
      "ENHANCER 2040 number of unique words 317 =? 317\n",
      "ENHANCER 335 number of unique words 82 =? 82\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_eid_list = []\n",
    "\n",
    "for row in [random.randint(0, len(df_ep)) for i in range(5)]:\n",
    "    test_eid_list.append(df_ep['enhancer'][row][0])\n",
    "\n",
    "for test_eid in test_eid_list:\n",
    "    L1 = len(features[test_eid].indices)\n",
    "\n",
    "    test_enh = getEnhancerById(df_ep, test_eid)\n",
    "    sent = test_enh[2]\n",
    "    \n",
    "    L2 = len(set(sent.split(' ')))\n",
    "    print('ENHANCER', test_eid, 'number of unique words', L1, '=?', L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE BINARY LABEL MATRIX (NxE)\n",
    "\n",
    "N = Number of nodes (enhancers + promoters)\n",
    "\n",
    "E = Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros(shape=(len(id_dict),2), dtype=float)\n",
    "\n",
    "for i in range(len(df_ep)):\n",
    "    eid = df_ep['enhancer'][i][0]\n",
    "    pid = df_ep['promoter'][i][0]\n",
    "    labels[eid] = [1,0] # enhancer class\n",
    "    labels[pid] = [0,1] # promoter class\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE FILES\n",
    "# ind.GM12878.allx = features\n",
    "# ind.GM12878.ally = labels\n",
    "# ind.GM12878.graph = adj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
